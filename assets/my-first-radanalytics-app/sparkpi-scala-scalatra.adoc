= SparkPi using Scala and Scalatra
:page-layout: markdown
:page-menu_template: menu_tutorial_application.html
:page-menu_backurl: /my-first-radanalytics-app.html
:page-menu_backtext: Back to My First RADanalytics Application

== Building a Spark PI Microservice with Scalatra

These instructions will help you to create a SparkPi microservice using https://www.scala-lang.org[Scala] and the http://scalatra.org[Scalatra] framework. In addition, this project will also demonstrate how to unit test your Spark jobs using the https://github.com/holdenk/spark-testing-base/wiki[Spark Testing Base] framework.

You should already have the necessary prerequisites installed and configured, but if not please review the link:/my-first-radanalytics-app.html[instructions]. In addition, this tutorial will assume you have used the Oshinko Web UI to create a Spark cluster named **spark**. This will be used as the **OSHINKO_CLUSTER_NAME** parameter.

== Create the Build

This project is built using https://www.scala-sbt.org/[SBT]. First, create the following file/directory structure

....
tutorial-sparkpi-scala-scalatra (root)
    \_ build.sbt
    \_ project
        \_ Dependencies.scala
        \_ build.properties
        \_ plugins.sbt
....

While the build.sbt is the primary build definition (analogous to pom.xml for Maven or build.gradle in Gradle), the project directory contains configurations and definitions that are applied project wide. This is a bit superfluous for a simple project like this, however, it is important when working with more complex multi-module builds.

The `build.properties` file will usually be autogenerated for you if you are using an IDE like Eclipse or IntelliJ IDEA. However, in this case it will be created by hand. In this example it simply contains the SBT version that the project is built with.
....
sbt.version = 1.0.2
....

The `plugins.sbt` file enables plugins globally for the project, and often adds extra settings and tasks. For this project, a few plugins to enable the **Scalatra** framework are added in addition to some plugins that enable the project to be packaged as a deployable "far jar".

....
logLevel := Level.Warn

resolvers += "Typesafe repository" at "http://repo.typesafe.com/typesafe/releases/"

addSbtPlugin( "com.eed3si9n" % "sbt-assembly" % "0.14.6" )
addSbtPlugin( "com.typesafe.sbt" % "sbt-native-packager" % "1.3.2" )
addSbtPlugin( "com.typesafe.sbt" % "sbt-twirl" % "1.3.13" )
addSbtPlugin( "org.scalatra.sbt" % "sbt-scalatra" % "1.0.2" )
....


The `Dependencies.scala` file is for dependency management. This centralizes all project dependencies and versions in one place. As mentioned above this is overkill for a simple project, however, for complex multi-module builds it is invaluable in preventing configuration drift across projects.

....
import sbt._

object Dependencies {

    val slf4jVersion = "1.7.5"
    val logbackVersion = "1.2.3"
    val sparkVersion = "2.3.0"
    val scalaTestVersion = "3.0.4"
    val scalatraVersion = "2.5.4"
    val jettyWebappVersion = "9.2.19.v20160908"
    val servletApiVersion = "3.1.0"
    val sparkTestBaseVersion = "2.2.0_0.8.0"

    val slf4j = Seq( "org.slf4j" % "slf4j-api" % slf4jVersion )

    val logback = Seq( "ch.qos.logback" % "logback-classic" % logbackVersion )

    val scalaTest = Seq( "org.scalatest" %% "scalatest" % scalaTestVersion % "test" )

    // TODO - fix versions later
    val scalatra = Seq( "org.scalatra" %% "scalatra" % scalatraVersion,
                        "org.scalatra" %% "scalatra-scalatest" % scalatraVersion % "test",
                        "org.eclipse.jetty" % "jetty-webapp" % jettyWebappVersion,
                        "javax.servlet" % "javax.servlet-api" % servletApiVersion )

    val spark = Seq( "org.apache.spark" %% "spark-core" % sparkVersion % "provided" )

    val sparkTestBase = Seq( "com.holdenkarau" %% "spark-testing-base" % sparkTestBaseVersion % "test" )


}
....

Finally, we come to the build definition itself. Comments have been added inline to the source code to explain the various components of the build.

....
import sbt._
import Dependencies._
organization := "io.radanalytics"
name := "tutorial-sparkpi-scala-scalatra"
version := "0.0.1-SNAPSHOT"
scalaVersion in ThisBuild := "2.11.11"

// 1. This is where SBT can reach out to resolve dependencies. SBT uses Apache Ivy to resolve dependencies by default but can be configured to reach out to Maven ones as well
resolvers += Resolver.sbtPluginRepo( "releases" )
resolvers += Classpaths.typesafeReleases
resolvers in ThisBuild ++= Seq( "Sonatype releases" at "https://oss.sonatype.org/content/repositories/releases",
                                "Spray IO Repository" at "http://repo.spray.io/",
                                "Maven Central" at "https://repo1.maven.org/maven2/",
                                "Typesafe repository" at "http://repo.typesafe.com/typesafe/releases/" )

// 2. Define the class to run when calling "java -jar ..."
mainClass in(Compile, run) := Some( "io.radanalytics.examples.scalatra.sparkpi.Main" )

// 3. Build the project to Java JAR conventions and add metadata to make Scalatra run
enablePlugins( JavaAppPackaging )
enablePlugins( ScalatraPlugin )


// 4. Add the project dependencies, see project/Dependencies.scala for dependency management
libraryDependencies ++= slf4j ++ logback ++ scalatra ++ scalaTest ++ spark ++ sparkTestBase


// 5. Deployment of this artifact should be part of a CI/CD pipeline. Running the unit tests while building the "fat jar" is very expensive,
//    therefore, don't do it during the "assembly" phase.
test in assembly := {}

// 6. Resolve any conflicts when merging into a "fat jar"
assemblyMergeStrategy in assembly := {
    case PathList( "META-INF", "MANIFEST.MF" ) => MergeStrategy.discard
    case PathList( "reference.conf" ) => MergeStrategy.concat
    //    case PathList( "META-INF", xs@_* ) => MergeStrategy.first
    case x => MergeStrategy.last
}
....

== Create the Spark Job


== Test the Spark Job
Thankfully, there is a library that will help enable the testing of Spark jobs in a unit test like environment. This framework, by using some utilities from https://github.com/apache/hadoop/tree/trunk/hadoop-minicluster[hadoop-minicluster], can stand up an entire Spark environment inside of a Scalatest fixture, execute jobs, and compare results. Unfortunately, this example relies on the computation based on a random number generator, which makes it extremely hard to unit test. However, for reference, the https://github.com/holdenk/spark-testing-base[Spark Testing Base] includes a slew of capabilities including https://github.com/holdenk/spark-testing-base/wiki/RDDComparisons[RDD Comparisons], https://github.com/holdenk/spark-testing-base/wiki/DataFrameSuiteBase[Data Frames], and https://github.com/holdenk/spark-testing-base/wiki/StreamingSuiteBase[Spark Streaming].

For context, a basic unit test would look something like this:
....
package io.radanalytics.examples.scalatra.sparkpi

import com.holdenkarau.spark.testing.SharedSparkContext
import org.scalatest.FlatSpec
import org.slf4j.{Logger, LoggerFactory}

class SparkPiTest extends FlatSpec with SharedSparkContext {

    val LOG : Logger = LoggerFactory.getLogger( this.getClass )

    "SparkPI" should "calculate to scale 2" in {
        val sparkPi : Double = new SparkPI( sc, 2 ).calculate()

        LOG.info( "--------------------------------------------" )
        LOG.info( s"---   Pi is roughly + $sparkPi" )
        LOG.info( "--------------------------------------------" )

        // NOTE - here is where you would put assertions, however, comparing floating point numbers that use random
        //        numbers in the algorithm is tricky so we don't do it here
        assert( true )
    }

}
....


== Implement the Service Endpoint
https://github.com/scalatra/scalatra[Scalatra] is designed from the ground up to be an approachable microservice framework. It is based on the http://sinatrarb.com[similarly named Ruby framework] with a Scala DSL and idioms. Setting up a SparkPI service is fairly easy and only requires the following:

1) Implementation of a servlet to handle requests. This handler will use the **SparkPi** calculation that was implemented in the previous step.
....
package io.radanalytics.examples.scalatra.sparkpi

import org.apache.spark.{SparkConf, SparkContext}
import org.scalatra.{Ok, ScalatraServlet}

class SparkPiServlet extends ScalatraServlet {
    get( "/sparkpi" ) {
        val spark = new SparkContext( new SparkConf().setAppName( "Radanalytics IO Scalatra Tutorial" ) )
        val sparkPi = new SparkPI( spark,2 ).calculate()
        println( sparkPi )
        spark.stop()
        Ok( "Pi is roughly " + sparkPi )
    }
}
....
2) Implementation of the Scalatra initialization. This class will override Scalatra's defaults as they rely too much on "magic" classpath naming and locations.
....
package io.radanalytics.examples.scalatra.sparkpi

import javax.servlet.ServletContext
import org.scalatra.LifeCycle

class ScalatraInit extends LifeCycle {

    override def init( context : ServletContext ) {
        context.mount( classOf[ SparkPiServlet ], "/*" )
    }

}
....
3) Implementing the **Main** class. This is the class that will be called when the application starts.
....
package io.radanalytics.examples.scalatra.sparkpi

import org.eclipse.jetty.server.Server
import org.eclipse.jetty.servlet.DefaultServlet
import org.eclipse.jetty.webapp.WebAppContext
import org.scalatra.servlet.ScalatraListener

object Main {

    def main( args : Array[ String ] ) : Unit = {
        val port = 8080 //TODO - do I need to make the port configurable/dynamic?
        val server = new Server( port )
        val context = new WebAppContext()

        context.setContextPath( "/" )
        context.setResourceBase( "src/main/webapp" )
        context.setInitParameter( ScalatraListener.LifeCycleKey, "io.radanalytics.examples.scalatra.sparkpi.ScalatraInit" ) // Scalatra uses a default that is not in line with best practices, so override it
        context.addEventListener( new ScalatraListener )
        context.addServlet( classOf[ DefaultServlet ], "/" ) // handles empty context root

        server.setHandler( context )
        server.start()
        server.join()
    }

}
....


== Deploy and Run the Application